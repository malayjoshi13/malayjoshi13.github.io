---
#layout: archive
permalink: /projects/
author_profile: true
---

<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/NMT_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="font-size: 14px;">Dissecting-Transformers: In-depth understanding and using Transformers</div>
		<div class="sub-title" style="font-size: 14px;">Started in Nov 2023 | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/Understanding-Transformer">Project1</a> | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/NeuralMachineTranslator">Project2</a></div><br>
		<span class="research-text" style="text-align: justify; display: inline-block; font-size: 14px; line-height: 1.5;">Tech: Vanilla Transformer, Pre-trained Language Models, Pytorch, Python, HuggingFace, Streamlit, Docker</span><br><br>			
		<span class="research-text" style="text-align: justify; display: inline-block; line-height: 1.5; font-size: 14px;">The first work is around building a <b>vanilla Transformer from scratch</b> and training it on iitb-english-hindi's sub-set of test dataset for the task of Machine Translation. Got a BLEU score of <b>0.61</b>, Character Error Rate of <b>0.16</b>, Word Error Rate of <b>0.35</b>, Train loss of <b>1.50</b> and Val loss of <b>1.53</b>. The second work is around building a <b>Machine Translation web app</b> using SoTA Encoder-Decoder-based <b>Pre-trained Language Models</b> for gaining hands-on experience working with LMs on a real case at the production level.</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/drishti_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="font-size: 14px;">Drishti: Visual Navigation Assistant for Visually Impaired</div>
		<div class="sub-title" style="font-size: 14px;">Started in Sept 2022 | <a target="_blank" class="tab_paper"  href="https://iopscience.iop.org/article/10.1088/1742-6596/2570/1/012032">Paper</a> | <a target="_blank" class="tab_paper"  href="/files/Drishti_Report.pdf">Project Report</a> </div><br>
		<span class="research-text" style="text-align: justify; display: inline-block; font-size: 14px; line-height: 1.5;">Tech: Computer Vision, Text-to-Speech, Google Cloud Platform (GCP), Python, TensorFlow, Electronics Design and Integrations, Microcontroller Programming </span><br><br>			
		<span class="research-text" style="text-align: justify; display: inline-block; line-height: 1.5; font-size: 14px;">Despite the development of numerous assistive devices over the years, due to various limitations, numerous <b>visually impaired individuals</b> in India still donâ€™t have a <b>navigational assistive tool/device</b>. After reviewing related literature, informal discussions with visually impaired individuals, and a formal survey conducted at Raghuveer Singh Memorial Blind Trust in Shahdara (Delhi), my comprehension of this problem significantly improved. To address it, I developed an initial-stage <b>low-cost eye-wear assistive device</b> and conducted a test with a group of visually impaired participants. This work started as <b>part of my Final-year College Project</b>, and I am actively working to improve this solution.</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/Pehchaan_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="font-size: 14px;">Pehchaan: Person Identifier for Auto-labeling Photographs</div>
		<div class="sub-title" style="font-size: 14px;">Started in July 2022 | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/Pehchaan">Project Page</a></div><br>
		<span class="research-text" style="text-align: justify; display: inline-block; font-size: 14px; line-height: 1.5;">Tech: Deep Learning, Face detection, Face recognition, Tensorflow, TensorBoard, Streamlit</span><br><br>			
		<span class="research-text" style="text-align: justify; display: inline-block; line-height: 1.5; font-size: 14px;"> Manually labelling people in a large stock of photographs is a very <b>time-consuming and labour-intensive</b> and without these labels, these significant photographs are mere pieces of memory/space-consuming items. Pehchaan is a <b>One-shot Labelling tool</b> which attempts to solve this issue by automatically identifying people present in photographs and then labelling their names in those photographs. This system uses pre-trained <b>Face detection</b> model, <b>Face alignment</b> model, <b>Face recognition</b> model and algorithms to keep checking if the database is modified and do <b>one-to-one matching</b> between feature representation of image input by the user and image(s) in the database. This work represents work done as part of my internship at DESIDOC-DRDO (New Delhi, India).</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/gsoc_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="font-size: 14px;">OligoFinder: Bio-NER System to Extract Oligonucleotide Entities</div>
		<div class="sub-title" style="font-size: 14px;">Started in June 2022 | <a target="_blank" class="tab_paper"  href="https://summerofcode.withgoogle.com/programs/2022/projects/5b96vIqa">Project Page</a></div><br>
		<span class="research-text" style="text-align: justify; display: inline-block; font-size: 14px; line-height: 1.5;">Tech: Pre-trained Language Model (BioBERT), Named Entity Recognition (NER), Python, TensorFlow, Google Cloud Platform (GCP), FastAPI</span><br><br>			
		<span class="research-text" style="text-align: justify; display: inline-block; line-height: 1.5; font-size: 14px;">Methods to extract textual references of oligonucleotides have remained limited to being a <b>time-consuming manual</b> process with the inability to generalize to newer variations. OligoFinder is developed as <b>part of the Google Summer of Code'22 program</b> at EMBL-EBI to address these limitations. It is a <b>scalable and semi-automated Bio-NER system</b> for identifying and extracting Oligonucleotide mentions and related data from Biomedical research papers.</span>
	</div>
</div>


<!-- <div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/teasers/faceoff.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">News-Shell</div>
		<div class="sub-title">Started in July 2022 | <a target="_blank" class="tab_paper"  href="add link">project page</a></div>
		<div class="sub-title">Tech: add tech </div>		
		<span class="research-text"> tell about project....extension of work "ShortRead" (add link of ShortRead project) started in Dec 2021.......</span>
	</div>
</div> -->


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/talkinghand_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="font-size: 14px;">TalkingHand: Sign Language Converter</div>
		<div class="sub-title" style="font-size: 14px;">Started in May 2021 | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/TalkingHand">Project Page</a></div><br>
		<span class="research-text" style="text-align: justify; display: inline-block; font-size: 14px; line-height: 1.5;">Tech: Computer Vision, Transfer-Learning, Python, TensorFlow </span><br><br>			
		<span class="research-text" style="text-align: justify; display: inline-block; line-height: 1.5; font-size: 14px;"> TalkingHand is a Computer Vision and Deep Learning-based <b>Sign Language to Text Conversion System</b> that, with the help of fine-tuned CNN of <b>VGG16</b>, classifies and converts the hand gestures made by the user into corresponding text-based labels. A <b>custom dataset of about 4000 images</b> each for 6 labels has been collected for fine-tuning the CNN model using a combination of <b>background subtraction</b> (BackgroundSubtractorMOG2) and <b>colour threshold techniques</b> so that data collected will have a lower bias due to the shape and colour of the user's hand making the gesture and altering lighting conditions. The model achieved a train loss of <b>0.0188</b>, train accuracy of <b>0.9965</b>, validation loss of <b>0.0913</b> and validation accuracy of <b>0.9888</b> after fine-tuning. The objective of this project is to take the first step towards developing a solution to help people with speaking and hearing disability communicate with other people. </span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/describer_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="font-size: 14px;">Describer: Image Captioning System</div>
		<div class="sub-title" style="font-size: 14px;">Started in Apr 2020 | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/Describer">Project Page</a></div><br>
		<span class="research-text" style="text-align: justify; display: inline-block; font-size: 14px; line-height: 1.5;">Tech: Computer Vision, NLP, Transfer-Learning, Python, TensorFlow </span><br><br>			
		<span class="research-text" style="text-align: justify; display: inline-block; line-height: 1.5; font-size: 14px;">Describer is an <b>Image Captioning System</b> that generates textual captions describing the images fed to it. It is trained on ***Flickr8k*** data. It uses pre-trained ***InceptionV3*** convolutional neural network which generates image embeddings and the ***GloVe's weight*** (having 6 Billion pairs of words and their corresponding 200 dim representational vectors) initialized Embedding layer which generates caption's word embeddings. Then, image embeddings go to a dense layer which compresses image embeddings into 256 dim and word embeddings go to ***LSTM*** recurrent neural network which outputs 256 dim representation. These two embeddings/representations then get added together and fed to the ***Feed-forward*** network which outputs the next word of the caption. Achieved ***BLEU-1 score of 0.79*** on the test dataset. The objective of this project is to take the first step towards developing a solution to help ***visually impaired people understand visual*** information around them.
		</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/Sanrakshan_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="font-size: 14px;">Sanrakshan: Animal Deterrent Device</div>
		<div class="sub-title" style="font-size: 14px;">Started in Jan 2020 | <a target="_blank" class="tab_paper" href="https://malayjoshi13.github.io/files/Sanrakshan_Report.pdf">Project Report</a> <a target="_blank" class="tab_paper" href="https://drive.google.com/file/d/1eC4c6zvbNNxLtWwpwbPqL4ohY22w0u4Z/view?usp=sharing">Video of Phase1</a> <a target="_blank" class="tab_paper" href="https://drive.google.com/file/d/1s_1gYTDBr2nosnFjSVyBHP34kpsHunBV/view?usp=sharing">Video of Phase2</a></div><br>
		<span class="research-text" style="text-align: justify; display: inline-block; font-size: 14px; line-height: 1.5;">Tech: Electronics Design and Integrations, Microcontroller Programming, Power Management, Mechanical Design</span><br><br>		
		<span class="research-text" style="text-align: justify; display: inline-block; line-height: 1.5; font-size: 14px;"> Farmers of Uttarakhand face the problem of crop destruction by wild animals. To tackle this issue Sanrakshan is developed. It's an animal deterrent device which prevents wild animals from destroying crops by <b>use of "Laser-LDR Detection Technology"</b>. This solution works on the principle of <b>using time-of-blocking</b> of laser light reaching the LDR sensor to differentiate between humans and target animals. This work results from extensive involvement in the <b>SIH'2020 competition</b> with two other team members, Abhay Jaiswal and Maitreyi.</span>
	</div>
</div>