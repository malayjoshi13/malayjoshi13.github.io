---
#layout: archive
permalink: /projects/
author_profile: true
---

<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/NMT_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="text-align: justify;">Dissecting-Transformers: In-depth understanding and using vanilla Transformers</div>
		<div class="sub-title" style="text-align: justify;">Started in Nov 2023 | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/Understanding-Transformer">Project Page for Project1</a> | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/NeuralMachineTranslator">Project Page for Project2</a></div>
		<span class="sub-title" style="text-align: justify; display: inline-block;">Tech: Language Models, Pytorch, Python, Tensorboard, HuggingFace, Streamlit, Docker</span>		
		<span class="research-text" style="text-align: justify; display: inline-block;">The first work is around building a <b>vanilla Transformer from scratch</b> and training it on iitb-english-hindi's sub-set of test dataset for the task of Machine Translation. Got a BLEU score of <b>0.61</b>, Character Error Rate of <b>0.16</b>, Word Error Rate of <b>0.35</b>, Train loss of <b>1.50</b> and Val loss of <b>1.53</b>. The second work is around building a <b>Machine Translation web app</b> using SoTA Encoder-Decoder-based <b>Pre-trained Language Models</b> for gaining hands-on experience working with LMs on a real case at the production level.</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/drishti_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="text-align: justify;">Drishti: Visual Navigation Assistant for Visually Impaired</div>
		<div class="sub-title" style="text-align: justify;">Started in Sept 2022 | <a target="_blank" class="tab_paper"  href="https://iopscience.iop.org/article/10.1088/1742-6596/2570/1/012032">Paper</a> | <a target="_blank" class="tab_paper"  href="/files/Drishti_Report.pdf">Project Report</a> </div>
		<span class="sub-title" style="text-align: justify; display: inline-block;">Tech: Computer Vision, Transfer Learning, Text-to-Speech, Google Cloud Platform (GCP), Python, TensorFlow, TensorBoard, Electronics Design and Integrations, Microcontroller Programming, Power Management, Mechanical Design </span>		
		<span class="research-text" style="text-align: justify; display: inline-block;">Despite the development of numerous assistive devices over the years, due to various limitations, numerous <b>visually impaired individuals</b> in India still donâ€™t have a <b>navigational assistive tool/device</b>. After reviewing related literature, informal discussions with visually impaired individuals, and a formal survey conducted at Raghuveer Singh Memorial Blind Trust in Shahdara (Delhi), my comprehension of this problem significantly improved. To address it, I developed an initial-stage <b>low-cost eye-wear assistive device</b> and conducted a test with a group of visually impaired participants. This work started as <b>part of my Final-year College Project</b>, and I am actively working to improve this solution.</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/Pehchaan_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="text-align: justify;">Pehchaan</div>
		<div class="sub-title" style="text-align: justify;">Started in July 2022 | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/Pehchaan">Project Page</a></div>
		<span class="sub-title" style="text-align: justify; display: inline-block;">Tech: Deep Learning, Face detection, Face recognition, Tensorflow, Streamlit</span>		
		<span class="research-text" style="text-align: justify; display: inline-block;"> Pehchaan is a <b>One-shot Labelling tool</b> to identify the name of the person present in an image. It uses pre-trained <b>Face detection</b> model, <b>Face alignment</b> model, <b>Face recognition</b> model and algorithms to keep checking if the database is modified and do <b>one-to-one matching</b> between feature representation of image input by the user and image(s) in the database. This tool attempts to automate labelling the people present in photographs. In its <b>absence, it's a very time-consuming and labour-intensive</b> task to label people present in a large stock of photographs manually, and without these labels, these significant photographs are mere pieces of memory/space-consuming items. This work represents work done as part of my internship at DESIDOC-DRDO (New Delhi, India).</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/gsoc_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="text-align: justify;">OligoFinder: Bio-NER System to Extract Oligonucleotide Entities</div>
		<div class="sub-title" style="text-align: justify;">Started in June 2022 | <a target="_blank" class="tab_paper"  href="https://summerofcode.withgoogle.com/programs/2022/projects/5b96vIqa">Project Page</a></div>
		<span class="sub-title" style="text-align: justify; display: inline-block;">Tech: Pre-trained Language Model (BioBERT), Named Entity Recognition (NER), Python, TensorFlow, Google Cloud Platform (GCP), TensorBoard, FastAPI</span>		
		<span class="research-text" style="text-align: justify; display: inline-block;">Methods to extract textual references of oligonucleotides have remained limited to being a <b>time-consuming manual</b> process with the inability to generalize to newer variations. OligoFinder is developed as <b>part of the Google Summer of Code'22 program</b> at EMBL-EBI to address these limitations. It is a <b>scalable and semi-automated Bio-NER system</b> for identifying and extracting Oligonucleotide mentions and related data from Biomedical research papers.</span>
	</div>
</div>


<!-- <div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/teasers/faceoff.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">News-Shell</div>
		<div class="sub-title">Started in July 2022 | <a target="_blank" class="tab_paper"  href="add link">project page</a></div>
		<div class="sub-title">Tech: add tech </div>		
		<span class="research-text"> tell about project....extension of work "ShortRead" (add link of ShortRead project) started in Dec 2021.......</span>
	</div>
</div> -->


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/talkinghand_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="text-align: justify;">TalkingHand: Sign Language Converter</div>
		<div class="sub-title" style="text-align: justify;">Started in May 2021 | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/TalkingHand">Project Page</a></div>
		<span class="sub-title" style="text-align: justify; display: inline-block;">Tech: Computer Vision, Transfer-Learning, Python, TensorFlow </span>		
		<span class="research-text" style="text-align: justify; display: inline-block;"> TalkingHand is a Computer Vision and Deep Learning-based <b>Sign Language to Text Conversion System</b> that, with the help of fine-tuned CNN of <b>VGG16</b>, classifies and converts the hand gestures made by the user into corresponding text-based labels. A <b>custom dataset of about 4000 images</b> each for 6 labels has been collected for fine-tuning the CNN model using a combination of <b>background subtraction</b> (BackgroundSubtractorMOG2) and <b>colour threshold techniques</b> so that data collected will have a lower bias due to the shape and colour of the user's hand making the gesture and altering lighting conditions. The model achieved a train loss of <b>0.0188</b>, train accuracy of <b>0.9965</b>, validation loss of <b>0.0913</b> and validation accuracy of <b>0.9888</b> after fine-tuning. The objective of this system is to take the first step towards developing a solution to help people with speaking and hearing disability communicate with other people. </span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/describer_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="text-align: justify;">Describer: Image Captioning System</div>
		<div class="sub-title" style="text-align: justify;">Started in Apr 2020 | <a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/Describer">Project Page</a></div>
		<span class="sub-title" style="text-align: justify; display: inline-block;">Tech: Computer Vision, NLP, Transfer-Learning, Flask, Python, TensorFlow </span>		
		<span class="research-text" style="text-align: justify;">Describer is an <b>Image Captioning System</b> that generates textual captions describing the images fed to it. This system consists of a <b>CNN model</b> (built from scratch) and an <b>LSTM model</b>. The <b>inceptionV3</b> model is used to generate image embeddings, and the <b>GloVe 200-dim model</b> is used to create embeddings of captions. The whole system is trained on Flickr8k data. Achieved BLEU-1 score of 0.79 on the test dataset. This system's objective is to take the first step towards developing a solution to help visually impaired people understand image-based information on their electronic devices and in their surroundings.
		</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/Sanrakshan_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title" style="text-align: justify;">Sanrakshan: Animal Deterrent Device</div>
		<div class="sub-title" style="text-align: justify;">Started in Jan 2020 | <a target="_blank" class="tab_paper" href="https://malayjoshi13.github.io/files/Sanrakshan_Report.pdf">Project Report</a> <a target="_blank" class="tab_paper" href="https://drive.google.com/file/d/1eC4c6zvbNNxLtWwpwbPqL4ohY22w0u4Z/view?usp=sharing">Video of Phase1</a> <a target="_blank" class="tab_paper" href="https://drive.google.com/file/d/1s_1gYTDBr2nosnFjSVyBHP34kpsHunBV/view?usp=sharing">Video of Phase2</a> </div>
		<span class="sub-title" style="text-align: justify; display: inline-block;">Tech: Electronics Design and Integrations, Microcontroller Programming, Power Management, Mechanical Design</span>		
		<span class="research-text" style="text-align: justify; display: inline-block;"> an animal deterrent device which prevents wild animals from destroying crops by <b>use of "Laser-LDR Detection Technology"</b>. This solution works on the principle of <b>using time-of-blocking</b> of laser light reaching the LDR sensor to differentiate between humans and target animals. This work results from extensive involvement in the <b>SIH'2020 competition</b> with two other team members, Abhay Jaiswal and Maitreyi.</span>
	</div>
</div>