---
#layout: archive
permalink: /projects/
author_profile: true
---

<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/teasers/faceoff.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">Understanding Transformer</div>
		<div class="sub-title">Started on Nov 2023<a target="_blank" class="tab_paper"  href="https://github.com/malayjoshi13/project_name">project page</a></div>
		<div class="sub-title">Tech: tell here </div>		
		<span class="research-text">tell about project</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/drishti_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">Drishti: Visual Navigation Assistant for Visually Impaired</div>
		<div class="sub-title">Started on Sept 2022 | <a target="_blank" class="tab_paper"  href="https://iopscience.iop.org/article/10.1088/1742-6596/2570/1/012032">Paper</a> | <a target="_blank" class="tab_paper"  href="/files/Drishti_Report.pdf">Project Report</a> </div>
		<div class="sub-title">Tech: Computer Vision, Transfer Learning, Text-to-Speech, Google Cloud Platform (GCP), Python, TensorFlow, TensorBoard, Electronics Design and Integrations, Microcontroller Programming, Power Management, Mechanical Design </div>		
		<span class="research-text">My mother being a special educator given me opportunity to have interactions with visual impaired students at her school. Throughout these interactions a recurring difficulty emerged that despite the development of numerous assistive devices over the years, due to various limitations (like being expensive, , inefficient functionality, internet/cloud based, obstructive physical setup, etc), still numerous visually impaired individuals in India don’t have a navigational assistive tool to entirely rely on. After related literature review, informal discussions with other visually impaired individuals and a formal survey conducted at Raghuveer Singh Memorial Blind Trust in Shahdara, Delhi, my comprehension significantly improved about this problem. To address it, an initial-stage low cost eye-wear assitive device is developed and it's successfull trials are conducted with a group of 8 participants from Raghuveer Singh Memorial Blind Trust. This work is started as part of my Final-year College Project and based on feedback received from participants, I am actively working to improve this solution to be used in real-world to assist visually impaired individuals in navigation.</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/teasers/faceoff.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">Pehchaan</div>
		<div class="sub-title">Started in July 2022 <a target="_blank" class="tab_paper"  href="add link">project page</a></div>
		<div class="sub-title">Tech: add tech </div>		
		<span class="research-text"> Representative of work done at DRDO.....</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/gsoc_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">OligoFinder: Bio-NER System to Extract Oligonucleotide Entities</div>
		<div class="sub-title">Started on June 2022 | <a target="_blank" class="tab_paper"  href="https://summerofcode.withgoogle.com/programs/2022/projects/5b96vIqa">Project Page</a></div>
		<div class="sub-title">Tech: Pre-trained Language Model (BioBERT), Named Entity Recognition (NER), Python, TensorFlow, Google Cloud Platform (GCP), TensorBoard, FastAPI</div>		
		<span class="research-text">Methods to extract textual-references of Oligonucleotide have remained limited to being time-consuming manual process with inability to generalize to newer variations. To address these limitations, OligoFinder is developed as part of Google Summer of Code'22 program at EMBL-EBI. It is a scalable and semi-automated Bio-NER system for identifying and extracting Oligonucleotide mentions along with related data from Biomedical research paper(s).</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/teasers/faceoff.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">News-Shell</div>
		<div class="sub-title">Started in July 2022 <a target="_blank" class="tab_paper"  href="add link">project page</a></div>
		<div class="sub-title">Tech: add tech </div>		
		<span class="research-text"> tell about project....extension of work "ShortRead" (add link of ShortRead project) started in Dec 2021.......</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/teasers/faceoff.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">TalkingHand: Sign Language Converter</div>
		<div class="sub-title">Started in May 2021 <a target="_blank" class="tab_paper"  href="add link">project page</a></div>
		<div class="sub-title">Tech: add tech </div>		
		<span class="research-text"> tell about project	</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/teasers/faceoff.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">Describer: Image Captioning System</div>
		<div class="sub-title">Started on Apr 2020 | <a target="_blank" class="tab_paper"  href="add link">project page</a></div>
		<div class="sub-title">Tech: Computer Vision, NLP, Transfer-Learning, Flask, Python, TensorFlow </div>		
		<span class="research-text">
		Describer is an image captioner which generates textual captions describing the images fed to it. This system consists of CNN model (built from scratch) and LSTM model. <b>InceptionV3</b> model is used to generate image embeddings and <b>GloVe</b> 200-dim model is used to generate embeddings of captions. Whole system is trained on Flickr8k data. Achieved test-BLEU-1 score of 0.47.
		</span>
	</div>
</div>


<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/Sanrakshan_gif.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">Sanrakshan: Animal Deterrent Device</div>
		<div class="sub-title">Started on Jan 2020 | <a target="_blank" class="tab_paper" href="https://malayjoshi13.github.io/files/Sanrakshan_Report.pdf">Project Report</a> <a target="_blank" class="tab_paper" href="https://drive.google.com/file/d/1eC4c6zvbNNxLtWwpwbPqL4ohY22w0u4Z/view?usp=sharing">Video of Phase1</a> <a target="_blank" class="tab_paper" href="https://drive.google.com/file/d/1s_1gYTDBr2nosnFjSVyBHP34kpsHunBV/view?usp=sharing">Video of Phase2</a> </div>
		<div class="sub-title">Tech: Electronics Design and Integrations, Microcontroller Programming, Power Management, Mechanical Design</div>		
		<span class="research-text"> Farmers of Uttarakhand face problem of crop destruction caused by wild animals. To address the limitations of existing solutions, which are either costly or limited in their functionality, we propose Sanrakshan, an animal deterrent device which prevents wild animals to destroy crops by use of “laser-LDR detection technology”. This solution, works on the principle of using time-of-blocking of laser light reaching LDR sensor to differentiates between humans and target animals (wild boar, deer, goat, buffalo and neelgayi). This project is done as part of SIH'2020 alongwith two other team members, Abhay Jaiswal and Maitreyi.</span>
	</div>
</div>